{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "text = load_pdf(\"sample.pdf\")\n",
    "\n",
    "print(text)  # preview"
   ],
   "id": "e66d52b2e9b015e2",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "chunks = chunk_text(text)   # <--- generate chunks here\n",
    "print(\"Number of chunks:\", len(chunks))"
   ],
   "id": "3873335e495f0e5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(chunks)\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))"
   ],
   "id": "d39dcfe678a9521f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def retrieve(query, top_k=3):\n",
    "    query_vec = model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_vec), top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n"
   ],
   "id": "a2c61ba1e8197d95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "def answer_question(query):\n",
    "    context = \"\\n\".join(retrieve(query))\n",
    "    prompt = f\"Answer the question based on context:\\nContext: {context}\\n\\nQuestion: {query}\"\n",
    "    return qa_model(prompt, max_length=200)[0][\"generated_text\"]\n"
   ],
   "id": "389673b05c61c5ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"New owner of drylab family?\"\n",
    "print(\"Q:\", query)\n",
    "print(\"A:\", answer_question(query))\n"
   ],
   "id": "eebae3490763a130"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Quality",
   "id": "834fe8cafaeca58a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "425b220df4e6a8cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# ----------------------------\n",
    "# 1. PDF LOADING + CHUNKING\n",
    "# ----------------------------\n",
    "def load_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=800, overlap=100):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks"
   ],
   "id": "78be86291802f91b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1. EMBEDDINGS + FAISS INDEX\n",
    "# ----------------------------\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "chat_history = []\n",
    "\n",
    "MODEL_MAX_TOKENS = 512       # Flan-T5 small/medium max input tokens\n",
    "MAX_NEW_TOKENS = 128\n",
    "\n",
    "def truncate_prompt_to_budget(prompt, tokenizer, max_input_tokens):\n",
    "    \"\"\"\n",
    "    Ensure final prompt fits within token budget\n",
    "    \"\"\"\n",
    "    ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    if len(ids) <= max_input_tokens:\n",
    "        return prompt\n",
    "    truncated = tokenizer.decode(ids[:max_input_tokens], skip_special_tokens=True)\n",
    "    return truncated\n",
    "MAX_INPUT_TOKENS = 512 - MAX_NEW_TOKENS  # model limit minus expected output\n",
    "\n",
    "def build_strict_context(retrieved_chunks, chat_history, tokenizer, history_limit=):\n",
    "    context_tokens = 0\n",
    "    context_text = \"\"\n",
    "\n",
    "    # 1) Add chunks until we hit token limit\n",
    "    for chunk in retrieved_chunks:\n",
    "        chunk_ids = tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        if context_tokens + len(chunk_ids) > MAX_INPUT_TOKENS - 50:  # reserve 50 for instructions\n",
    "            remaining = MAX_INPUT_TOKENS - 50 - context_tokens\n",
    "            if remaining > 0:\n",
    "                context_text += tokenizer.decode(chunk_ids[:remaining], skip_special_tokens=True)\n",
    "            break\n",
    "        context_text += chunk + \"\\n\"\n",
    "        context_tokens += len(chunk_ids)\n",
    "\n",
    "    # 2) Add history if it fits\n",
    "    history_text = \"\"\n",
    "    for q, a in chat_history[-history_limit:]:\n",
    "        qa_text = f\"Q: {q}\\nA: {a}\\n\"\n",
    "        qa_ids = tokenizer.encode(qa_text, add_special_tokens=False)\n",
    "        if context_tokens + len(qa_ids) > MAX_INPUT_TOKENS - 50:\n",
    "            break\n",
    "        history_text += qa_text\n",
    "        context_tokens += len(qa_ids)\n",
    "\n",
    "    return history_text, context_text\n"
   ],
   "id": "fa9625d3418d99c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 2. QA MODEL WITH MEMORY & CLEAN CONTEXT\n",
    "# ----------------------------\n",
    "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=0)\n",
    "\n",
    "def answer_question_token_safe(query, index, chunks, history_limit=2, top_k=3):\n",
    "    # 1) retrieve top-k relevant chunks\n",
    "    retrieved_chunks = retrieve(query, index, chunks, top_k=top_k)\n",
    "\n",
    "    # 2) build token-safe context\n",
    "    reserved_tokens_for_instructions = 50\n",
    "    context_budget = MAX_INPUT_TOKENS - reserved_tokens_for_instructions\n",
    "    context, used_tokens = build_context_token_safe(retrieved_chunks, tokenizer, context_budget)\n",
    "\n",
    "    # 3) prepare chat history (multi-turn)\n",
    "    history_text = \"\"\n",
    "    history_token_count = 0\n",
    "    for q, a in chat_history[-history_limit:]:\n",
    "        pair_text = f\"Q: {q}\\nA: {a}\\n\"\n",
    "        pair_tokens = len(tokenizer.encode(pair_text, add_special_tokens=False))\n",
    "        if history_token_count + pair_tokens > context_budget - used_tokens:\n",
    "            break\n",
    "        history_text += pair_text\n",
    "        history_token_count += pair_tokens\n",
    "\n",
    "    # 4) assemble full prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant.\n",
    "Previous conversation:\n",
    "{history_text}\n",
    "Answer the question ONLY using the context below.\n",
    "If the answer is not in the context, say \"I don't know from the document.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "\n",
    "    # 5) truncate prompt if still over budget\n",
    "    prompt = truncate_prompt_to_budget(prompt, tokenizer, MAX_INPUT_TOKENS)\n",
    "\n",
    "    # 6) generate answer\n",
    "    response = qa_model(\n",
    "        prompt,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # 7) save in history and return\n",
    "    chat_history.append((query, response))\n",
    "    return response\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage\n",
    "# ----------------------------\n",
    "# chunks = [\"Your PDF chunks here...\"]\n",
    "# index, _ = build_faiss_index(chunks)\n",
    "\n",
    "print(answer_question_token_safe(\"What dataset was used?\", index, chunks))\n",
    "print(answer_question_token_safe(\"What problem do Sparse Feature Networks aim to solve?\", index, chunks))\n",
    "print(answer_question_token_safe(\"How does that relate to the dataset?\", index, chunks))\n"
   ],
   "id": "5f35bb24c777fc95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e1505f682fb169d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 4. MAIN EXECUTION\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # load and chunk PDF\n",
    "    pdf_text = load_pdf(\"sample.pdf\")   # ðŸ”¹ replace with your PDF\n",
    "    chunks = chunk_text(pdf_text, chunk_size=800, overlap=100)\n",
    "\n",
    "    # build FAISS\n",
    "    index, vectors = build_faiss_index(chunks)\n",
    "\n",
    "    # ask a question\n",
    "    query = \"What problem do Sparse Feature Networks aim to solve ?\"\n",
    "    print(\"Q:\", query)\n",
    "    print(\"A:\", answer_question(query, index, chunks))\n"
   ],
   "id": "919e643145db8d4d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
