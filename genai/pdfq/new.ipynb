{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-19T11:42:47.962507Z",
     "start_time": "2025-08-19T11:42:23.481156Z"
    }
   },
   "source": [
    "# ----------------------------\n",
    "# 0. Imports\n",
    "# ----------------------------\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1. PDF LOADING + CHUNKING\n",
    "# ----------------------------\n",
    "def load_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits text into smaller chunks of ~chunk_size tokens\n",
    "    with some overlap for context continuity.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# ----------------------------\n",
    "# 2. EMBEDDINGS + FAISS INDEX\n",
    "# ----------------------------\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "chat_history = []\n",
    "\n",
    "MODEL_MAX_TOKENS = 512\n",
    "MAX_NEW_TOKENS = 128\n",
    "MAX_INPUT_TOKENS = MODEL_MAX_TOKENS - MAX_NEW_TOKENS\n",
    "\n",
    "def build_faiss_index(chunks):\n",
    "    vectors = embedder.encode(chunks, convert_to_numpy=True)\n",
    "    dim = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(vectors)\n",
    "    return index, vectors\n",
    "\n",
    "def retrieve(query, index, chunks, top_k=3):\n",
    "    query_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# ----------------------------\n",
    "# 3. TOKEN-SAFE CONTEXT BUILDER\n",
    "# ----------------------------\n",
    "def build_context_token_safe(retrieved_chunks, chat_history, tokenizer, history_limit=3):\n",
    "    context_tokens = 0\n",
    "    context_text = \"\"\n",
    "\n",
    "    # 1) Add retrieved chunks safely\n",
    "    for chunk in retrieved_chunks:\n",
    "        chunk_ids = tokenizer.encode(chunk, add_special_tokens=False)\n",
    "        if context_tokens + len(chunk_ids) > MAX_INPUT_TOKENS - 50:  # reserve 50 for instructions\n",
    "            remaining = MAX_INPUT_TOKENS - 50 - context_tokens\n",
    "            if remaining > 0:\n",
    "                context_text += tokenizer.decode(chunk_ids[:remaining], skip_special_tokens=True)\n",
    "            break\n",
    "        context_text += chunk + \"\\n\"\n",
    "        context_tokens += len(chunk_ids)\n",
    "\n",
    "    # 2) Add recent chat history safely\n",
    "    history_text = \"\"\n",
    "    for q, a in chat_history[-history_limit:]:\n",
    "        qa_text = f\"Q: {q}\\nA: {a}\\n\"\n",
    "        qa_ids = tokenizer.encode(qa_text, add_special_tokens=False)\n",
    "        if context_tokens + len(qa_ids) > MAX_INPUT_TOKENS - 50:\n",
    "            break\n",
    "        history_text += qa_text\n",
    "        context_tokens += len(qa_ids)\n",
    "\n",
    "    return history_text, context_text\n",
    "\n",
    "def truncate_prompt_to_budget(prompt, tokenizer, max_input_tokens):\n",
    "    ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    if len(ids) <= max_input_tokens:\n",
    "        return prompt\n",
    "    return tokenizer.decode(ids[:max_input_tokens], skip_special_tokens=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. QA MODEL\n",
    "# ----------------------------\n",
    "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=0)\n",
    "\n",
    "def answer_question_token_safe(query, index, chunks, history_limit=2, top_k=3):\n",
    "    retrieved_chunks = retrieve(query, index, chunks, top_k=top_k)\n",
    "    history_text, context_text = build_context_token_safe(retrieved_chunks, chat_history, tokenizer, history_limit=history_limit)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant.\n",
    "Previous conversation:\n",
    "{history_text}\n",
    "Answer the question ONLY using the context below.\n",
    "If the answer is not in the context, say \"I don't know from the document.\"\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "    prompt = truncate_prompt_to_budget(prompt, tokenizer, MAX_INPUT_TOKENS)\n",
    "\n",
    "    response = qa_model(prompt, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    chat_history.append((query, response))\n",
    "    return response\n",
    "\n",
    "# ----------------------------\n",
    "# 5. USAGE EXAMPLE\n",
    "# ----------------------------\n",
    "pdf_text = load_pdf(\"sample.pdf\")\n",
    "chunks = chunk_text(pdf_text)\n",
    "index, _ = build_faiss_index(chunks)\n",
    "\n",
    "queries = [\n",
    "    \"What dataset was used?\",\n",
    "    \"What problem do Sparse Feature Networks aim to solve?\",\n",
    "    \"How does that relate to the dataset?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(answer_question_token_safe(q, index, chunks))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legacy Survey website(Dey et al. 2019). Our SDSS sample comprises 250,207 galaxies in total. Our SFNet implementation is shown in Figure 1. We use a CNN architecture identical to the resnet18 (He et al. 2015), except that we insert a top- koperation be- fore the final linear layer. This additional top- klayer guarantees that each prediction is a linear combination ofksparse features. We use d= 512 latent features (af- ter the average pooling layer) and\n",
      "I don't know from the document.\n",
      "We surmise that regular CNN features are not in- terpretable. Meanwhile, our SFNet is both performant and interpretable. Finally, we check whether the SFNet can capture in- formation not found within the 512-dimensional Zoobot feature vector. We optimize a ridge regression model to predict SFNet feature values using the Zoobot features, finding R2values of 0 .253 and 0 .107 for the two primary SFNet features, Z 61andZ 256 , respectively. These low coefficients of determination suggest that our SFNet ex\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
