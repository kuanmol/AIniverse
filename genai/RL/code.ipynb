{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:03:50.588167Z",
     "start_time": "2025-08-25T16:03:47.556170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import DPOTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})  # Add pad token explicitly\n",
    "    print(\"Pad token set to EOS token:\", tokenizer.pad_token)\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Sync model with tokenizer pad token\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Create preference dataset\n",
    "def create_preference_dataset():\n",
    "    samples = [\n",
    "        {\"prompt\": \"What is the capital of France?\", \"chosen\": \"The capital of France is Paris.\", \"rejected\": \"I don't know about France.\"},\n",
    "        {\"prompt\": \"How to make tea?\", \"chosen\": \"To make tea, boil water and steep tea leaves for 3-5 minutes.\", \"rejected\": \"Tea is made with coffee beans.\"},\n",
    "        {\"prompt\": \"What is 2+2?\", \"chosen\": \"2 + 2 equals 4.\", \"rejected\": \"2 + 2 is 5.\"},\n",
    "        {\"prompt\": \"Translate 'hello' to Spanish:\", \"chosen\": \"Hello in Spanish is 'hola'.\", \"rejected\": \"Hello in Spanish is 'bonjour'.\"},\n",
    "        {\"prompt\": \"What is Python?\", \"chosen\": \"Python is a popular programming language.\", \"rejected\": \"Python is a type of snake.\"},\n",
    "    ]\n",
    "    return Dataset.from_list(samples)\n",
    "\n",
    "# Preprocess dataset for DPO\n",
    "def preprocess_dpo_dataset(dataset, tokenizer, max_length=128, max_prompt_length=64):\n",
    "    def tokenize_example(example):\n",
    "        prompt_tokens = tokenizer(example[\"prompt\"], truncation=True, max_length=max_prompt_length, padding=False)\n",
    "        chosen_tokens = tokenizer(example[\"chosen\"], truncation=True, max_length=max_length, padding=False)\n",
    "        rejected_tokens = tokenizer(example[\"rejected\"], truncation=True, max_length=max_length, padding=False)\n",
    "        return {\n",
    "            \"prompt_input_ids\": prompt_tokens[\"input_ids\"],\n",
    "            \"prompt_attention_mask\": prompt_tokens[\"attention_mask\"],\n",
    "            \"chosen_input_ids\": chosen_tokens[\"input_ids\"],\n",
    "            \"chosen_attention_mask\": chosen_tokens[\"attention_mask\"],\n",
    "            \"rejected_input_ids\": rejected_tokens[\"input_ids\"],\n",
    "            \"rejected_attention_mask\": rejected_tokens[\"attention_mask\"],\n",
    "        }\n",
    "    return dataset.map(tokenize_example, remove_columns=[\"prompt\", \"chosen\", \"rejected\"])\n",
    "\n",
    "# Create and preprocess dataset\n",
    "train_dataset = create_preference_dataset()\n",
    "train_dataset = preprocess_dpo_dataset(train_dataset, tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dpo-lora-model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=1,\n",
    "    save_steps=50,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "# Create DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,  # DPO can work without a reference model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    max_length=128,\n",
    "    max_prompt_length=64,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting DPO training with QLoRA...\")\n",
    "dpo_trainer.train()\n",
    "\n",
    "# Save the model\n",
    "dpo_trainer.save_model()\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Test the model\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "test_prompt = \"What is the capital of Germany?\"\n",
    "response = generate_response(test_prompt)\n",
    "print(f\"\\nPrompt: {test_prompt}\")\n",
    "print(f\"Response: {response}\")"
   ],
   "id": "2971a50d1220adef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "GPU memory: 6.0 GB\n",
      "Pad token set to EOS token: <|endoftext|>\n",
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b086d2dd4aaf4155807a70bdf772ca1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "DPOTrainer.__init__() got an unexpected keyword argument 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 106\u001B[39m\n\u001B[32m     92\u001B[39m training_args = TrainingArguments(\n\u001B[32m     93\u001B[39m     output_dir=\u001B[33m\"\u001B[39m\u001B[33m./dpo-lora-model\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     94\u001B[39m     per_device_train_batch_size=\u001B[32m1\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    102\u001B[39m     report_to=[],\n\u001B[32m    103\u001B[39m )\n\u001B[32m    105\u001B[39m \u001B[38;5;66;03m# Create DPO trainer\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m dpo_trainer = \u001B[43mDPOTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    108\u001B[39m \u001B[43m    \u001B[49m\u001B[43mref_model\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# DPO can work without a reference model\u001B[39;49;00m\n\u001B[32m    109\u001B[39m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    110\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    111\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m128\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    112\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_prompt_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m64\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[32m    116\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting DPO training with QLoRA...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mTypeError\u001B[39m: DPOTrainer.__init__() got an unexpected keyword argument 'max_length'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SFT + DPO with QLoRA (single script)\n",
    "# Run on RTX 3050 (6GB). Adjust batch/accumulation if OOM.\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# -------------------------\n",
    "# Settings (tweak these)\n",
    "# -------------------------\n",
    "model_id = \"distilgpt2\"   # small for prototyping\n",
    "OUTPUT_DIR = \"./sft_then_dpo_adapters\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert device == \"cuda\", \"CUDA required for this script.\"\n",
    "\n",
    "# Training hyperparams\n",
    "SFT_BATCH = 1\n",
    "SFT_GRAD_ACC = 8\n",
    "SFT_EPOCHS = 2\n",
    "SFT_MAX_LEN = 128\n",
    "\n",
    "DPO_BATCH = 1\n",
    "DPO_GRAD_ACC = 8\n",
    "DPO_EPOCHS = 2\n",
    "DPO_MAX_LEN = 128\n",
    "\n",
    "# -------------------------\n",
    "# 1) Synthetic dataset\n",
    "# -------------------------\n",
    "def make_sft_examples():\n",
    "    sft = [\n",
    "        {\"prompt\": \"Translate 'Hello' to French:\", \"response\": \"Bonjour\"},\n",
    "        {\"prompt\": \"Translate 'Thank you' to French:\", \"response\": \"Merci\"},\n",
    "        {\"prompt\": \"What is 2+2?\", \"response\": \"4\"},\n",
    "        {\"prompt\": \"What is 7+5?\", \"response\": \"12\"},\n",
    "        {\"prompt\": \"Summarize: Photosynthesis is the process by which green plants use sunlight.\", \"response\": \"Plants convert sunlight into chemical energy to make food.\"},\n",
    "        {\"prompt\": \"Translate 'Good morning' to German:\", \"response\": \"Guten Morgen\"},\n",
    "        {\"prompt\": \"Translate 'I love you' to Italian:\", \"response\": \"Ti amo\"},\n",
    "        {\"prompt\": \"What is 3x3?\", \"response\": \"9\"},\n",
    "        {\"prompt\": \"Translate 'Goodbye' to Spanish:\", \"response\": \"Adiós\"},\n",
    "        {\"prompt\": \"Summarize Newton's first law in one sentence.\", \"response\": \"An object in motion stays in motion unless acted on by an external force.\"},\n",
    "    ]\n",
    "    big = sft * 20   # 200 examples\n",
    "    random.shuffle(big)\n",
    "    return big\n",
    "\n",
    "def make_dpo_pairs_from_sft(sft_list):\n",
    "    pairs = []\n",
    "    for ex in sft_list:\n",
    "        prompt = ex[\"prompt\"]\n",
    "        chosen = ex[\"response\"]\n",
    "        if chosen.isdigit():\n",
    "            rejected = str(int(chosen) + random.choice([1, -1, 2]))\n",
    "        else:\n",
    "            rejected = \"Nope\"\n",
    "        pairs.append({\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected})\n",
    "    return pairs\n",
    "\n",
    "sft_examples = make_sft_examples()\n",
    "dpo_pairs = make_dpo_pairs_from_sft(sft_examples)\n",
    "\n",
    "# train/eval splits\n",
    "sft_train = sft_examples[:160]\n",
    "sft_eval = sft_examples[160:]\n",
    "dpo_train = dpo_pairs[:160]\n",
    "dpo_eval = dpo_pairs[160:]\n",
    "\n",
    "train_sft_ds = Dataset.from_list(sft_train)\n",
    "eval_sft_ds = Dataset.from_list(sft_eval)\n",
    "train_dpo_ds = Dataset.from_list(dpo_train)\n",
    "eval_dpo_ds = Dataset.from_list(dpo_eval)\n",
    "\n",
    "print(\"SFT train:\", len(train_sft_ds), \"eval:\", len(eval_sft_ds))\n",
    "print(\"DPO train:\", len(train_dpo_ds), \"eval:\", len(eval_dpo_ds))\n",
    "\n",
    "# -------------------------\n",
    "# 2) Tokenizer + 4-bit config\n",
    "# -------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# -------------------------\n",
    "# 3) Load base model (4-bit) and prepare for k-bit training\n",
    "# -------------------------\n",
    "print(\"Loading base model in 4-bit (may take a bit)...\")\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Attach LoRA adapters for SFT\n",
    "# -------------------------\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 style\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "print(\"LoRA params (trainable):\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# -------------------------\n",
    "# 5) SFT: tokenization\n",
    "# -------------------------\n",
    "def sft_tokenize(example):\n",
    "    prompt = example[\"prompt\"].strip()\n",
    "    resp = example[\"response\"].strip()\n",
    "    text = prompt + \" \" + resp\n",
    "    toks = tokenizer(text, truncation=True, max_length=SFT_MAX_LEN, padding=\"max_length\")\n",
    "    input_ids = toks[\"input_ids\"]\n",
    "    prompt_ids = tokenizer(prompt, truncation=True, max_length=SFT_MAX_LEN)[\"input_ids\"]\n",
    "    prompt_len = len(prompt_ids)\n",
    "    labels = [-100] * prompt_len + input_ids[prompt_len:]\n",
    "    labels = labels[: len(input_ids)]\n",
    "    if len(labels) < len(input_ids):\n",
    "        labels += [-100] * (len(input_ids) - len(labels))\n",
    "    toks[\"labels\"] = labels\n",
    "    return toks\n",
    "\n",
    "train_tok = train_sft_ds.map(sft_tokenize, remove_columns=train_sft_ds.column_names, batched=False)\n",
    "eval_tok = eval_sft_ds.map(sft_tokenize, remove_columns=eval_sft_ds.column_names, batched=False)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# -------------------------\n",
    "# 6) SFT training\n",
    "# -------------------------\n",
    "sft_args = TrainingArguments(\n",
    "    output_dir=\"./sft_qLora_out\",\n",
    "    per_device_train_batch_size=SFT_BATCH,\n",
    "    gradient_accumulation_steps=SFT_GRAD_ACC,\n",
    "    num_train_epochs=SFT_EPOCHS,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Starting SFT (QLoRA) ...\")\n",
    "trainer.train()\n",
    "print(\"SFT done — saving adapters...\")\n",
    "model.save_pretrained(os.path.join(OUTPUT_DIR, \"sft_adapters\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"sft_adapters\"))\n",
    "\n",
    "# -------------------------\n",
    "# 7) Reload base model + SFT adapters for DPO\n",
    "# -------------------------\n",
    "print(\"Reloading base model and applying SFT adapters for DPO step...\")\n",
    "base2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "model_with_adapters = PeftModel.from_pretrained(base2, os.path.join(OUTPUT_DIR, \"sft_adapters\"))\n",
    "\n",
    "# IMPORTANT: enable training\n",
    "model_with_adapters.train()\n",
    "\n",
    "# sanity check: print trainable params\n",
    "print(\"LoRA params (trainable after reload):\",\n",
    "      sum(p.numel() for p in model_with_adapters.parameters() if p.requires_grad))\n",
    "\n",
    "# -------------------------\n",
    "# 8) DPO config + Trainer\n",
    "# -------------------------\n",
    "dpo_args = DPOConfig(\n",
    "    output_dir=\"./dpo_from_sft_adapters\",\n",
    "    per_device_train_batch_size=DPO_BATCH,\n",
    "    per_device_eval_batch_size=DPO_BATCH,\n",
    "    gradient_accumulation_steps=DPO_GRAD_ACC,\n",
    "    num_train_epochs=DPO_EPOCHS,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[],\n",
    "\n",
    "    beta=0.1,\n",
    "    max_length=DPO_MAX_LEN,\n",
    "    max_prompt_length=64,\n",
    "    padding_value=tokenizer.pad_token_id,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    ")\n",
    "\n",
    "print(\"Starting DPO training (preference optimization)...\")\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model_with_adapters,\n",
    "    ref_model=None,   # TRL will clone frozen ref internally\n",
    "    args=dpo_args,\n",
    "    train_dataset=train_dpo_ds,\n",
    "    eval_dataset=eval_dpo_ds,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "dpo_trainer.train()\n",
    "print(\"DPO done — saving final adapters...\")\n",
    "dpo_trainer.model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_adapters\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_adapters\"))\n",
    "\n",
    "# -------------------------\n",
    "# 9) Reload for inference\n",
    "# -------------------------\n",
    "print(\"Reloading final model + adapters for inference...\")\n",
    "base_inf = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "final_model = PeftModel.from_pretrained(base_inf, os.path.join(OUTPUT_DIR, \"final_adapters\"))\n",
    "final_model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# 10) Inference helper\n",
    "# -------------------------\n",
    "def generate_answer(prompt: str, max_new_tokens=40):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(final_model.device)\n",
    "    with torch.no_grad():\n",
    "        out = final_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return decoded[len(prompt):].strip() if decoded.startswith(prompt) else decoded.strip()\n",
    "\n",
    "# Quick tests\n",
    "tests = [\n",
    "    \"Translate 'Hello' to French:\",\n",
    "    \"What is 7 + 5?\",\n",
    "    \"Summarize: Photosynthesis is the process by which green plants use sunlight.\"\n",
    "]\n",
    "for t in tests:\n",
    "    print(\"PROMPT:\", t)\n",
    "    print(\"ANSWER:\", generate_answer(t))\n",
    "    print(\"-\" * 40)\n"
   ],
   "id": "ab324c8d9dd84600",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
