{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T13:50:10.026464Z",
     "start_time": "2025-08-22T13:48:05.871694Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load a small model (Flan-T5 for answers)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "qa_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From R:\\AIniverse\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T13:53:07.131100Z",
     "start_time": "2025-08-22T13:53:07.125148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ------------------------\n",
    "# Conversational Memory\n",
    "# ------------------------\n",
    "chat_history = []  # stores (user, bot) pairs\n",
    "\n",
    "def build_prompt(user_query):\n",
    "    \"\"\"Build prompt including chat history.\"\"\"\n",
    "    history_text = \"\"\n",
    "    for u, b in chat_history[-3:]:  # keep last 3 turns\n",
    "        history_text += f\"User: {u}\\nBot: {b}\\n\"\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "Conversation so far:\n",
    "{history_text}\n",
    "\n",
    "Now answer the new user question as user is PHD in computer science:\n",
    "User: {user_query}\n",
    "Bot:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def generate_answer(user_query):\n",
    "    # 1. Build prompt with history\n",
    "    prompt = build_prompt(user_query)\n",
    "\n",
    "    # 2. Generate answer\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = qa_model.generate(**inputs, max_length=150)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 3. Save to memory\n",
    "    chat_history.append((user_query, answer))\n",
    "\n",
    "    return answer"
   ],
   "id": "c3406bd9d84ddeab",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T13:53:13.916369Z",
     "start_time": "2025-08-22T13:53:08.204596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ------------------------\n",
    "# Example Conversation\n",
    "# ------------------------\n",
    "print(\"User: Hello, who are you?\")\n",
    "print(\"Bot:\", generate_answer(\"Hello, who are you?\"))\n",
    "\n",
    "print(\"\\nUser: Can you tell me about Python?\")\n",
    "print(\"Bot:\", generate_answer(\"Can you tell me about Python?\"))\n",
    "\n",
    "print(\"\\nUser: And what about Java?\")\n",
    "print(\"Bot:\", generate_answer(\"And what about Java?\"))\n",
    "\n",
    "print(\"\\nUser: Compare Python and Java in short.\")\n",
    "print(\"Bot:\", generate_answer(\"Compare Python and Java in short.\"))\n"
   ],
   "id": "223c48dc5591f1c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello, who are you?\n",
      "Bot: I am Bot.\n",
      "\n",
      "User: Can you tell me about Python?\n",
      "Bot: I am Bot. I am a computer science student. I am interested in learning about Python.\n",
      "\n",
      "User: And what about Java?\n",
      "Bot: I am interested in learning about Java.\n",
      "\n",
      "User: Compare Python and Java in short.\n",
      "Bot: I am Bot. I am a computer science student. I am interested in learning about Python and Java.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T11:51:13.952828Z",
     "start_time": "2025-08-22T11:49:06.968741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install transformers accelerate torch --upgrade\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"google/flan-t5-large\"  # stronger than base; switch to base if needed\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16 if DEVICE==\"cuda\" else None)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ------------------------\n",
    "# Minimal memory store\n",
    "# ------------------------\n",
    "chat_history = []  # list of (user, bot)\n",
    "\n",
    "def add_turn(user_msg: str, bot_msg: str):\n",
    "    chat_history.append((user_msg, bot_msg))\n",
    "\n",
    "def last_k_history(k=6):\n",
    "    return chat_history[-k:]\n",
    "\n",
    "def extract_relevant_facts(keywords):\n",
    "    \"\"\"Return prior bot statements that mention any keyword (case-insensitive).\"\"\"\n",
    "    kws = [k.lower() for k in keywords]\n",
    "    facts = []\n",
    "    for u, b in chat_history:\n",
    "        if any(k in u.lower() or k in b.lower() for k in kws):\n",
    "            facts.append(f\"- From earlier: {b}\")\n",
    "    return \"\\n\".join(facts) if facts else \"- (no prior facts found)\"\n",
    "\n",
    "def build_prompt(user_query: str, force_keywords=None):\n",
    "    \"\"\"\n",
    "    Strong prompt: shows short recent convo + a focused 'Facts' block.\n",
    "    We explicitly instruct the model to use ALL facts.\n",
    "    \"\"\"\n",
    "    # recent convo (for continuity)\n",
    "    hist = \"\\n\".join([f\"User: {u}\\nBot: {b}\" for u, b in last_k_history(6)])\n",
    "\n",
    "    # targeted facts (to avoid the model latching onto only one)\n",
    "    fact_block = \"\"\n",
    "    if force_keywords:\n",
    "        fact_block = extract_relevant_facts(force_keywords)\n",
    "\n",
    "    prompt = f\"\"\"You are a careful assistant. Always ground your answer in the provided facts and conversation.\n",
    "\n",
    "Conversation (recent):\n",
    "{hist if hist else \"(no prior conversation)\"}\n",
    "\n",
    "Facts extracted from conversation (use ALL of them; do not ignore or overwrite):\n",
    "{fact_block if fact_block else \"- (none)\"}\n",
    "\n",
    "User question: {user_query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Synthesize ALL relevant facts above.\n",
    "- If comparing two items (e.g., Python vs Java), reference BOTH explicitly.\n",
    "- Be concise and avoid repeating exact prior sentences verbatim.\n",
    "- If facts are contradictory or unusual, still respect them (they come from the user's context).\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(user_query: str, force_keywords=None, max_new_tokens=160, temperature=0.4, top_p=0.9, repetition_penalty=1.1):\n",
    "    prompt = build_prompt(user_query, force_keywords=force_keywords)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    ans = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    add_turn(user_query, ans)\n",
    "    return ans\n",
    "\n",
    "# ------------------------\n",
    "# Demo 1: Fake-history proof (forces use of BOTH facts)\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Clear history\n",
    "    chat_history.clear()\n",
    "\n",
    "    # Inject fake knowledge (to prove history is read AND combined)\n",
    "    add_turn(\"What is Python?\", \"Python is a fruit found in India.\")\n",
    "    add_turn(\"What is Java?\", \"Java is a traditional dance performed at festivals.\")\n",
    "\n",
    "    print(\"User: Compare Python and Java in short.\")\n",
    "    ans = generate(\n",
    "        \"Compare Python and Java in short.\",\n",
    "        force_keywords=[\"Python\", \"Java\"]   # <- ensures both facts are extracted\n",
    "    )\n",
    "    print(\"Bot:\", ans)"
   ],
   "id": "dbf84d6e2ac6ccd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2693ec6f5dad46079ee9ca4431fddfbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf6119621df04cffa54e80c64644482f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Compare Python and Java in short.\n",
      "Bot: If comparing two items (e.g., Python vs Java), reference BOTH explicitly.\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
