{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T17:25:11.324564Z",
     "start_time": "2025-08-23T17:25:06.097261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import PyPDF2\n",
    "\n",
    "    HAS_PYPDF2 = True\n",
    "except Exception:\n",
    "    HAS_PYPDF2 = False\n",
    "\n",
    "LLM_NAME = \"google/flan-t5-base\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "top_k = 2\n",
    "DEBUG = True\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(LLM_NAME)\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "knowledge_chunks = [\n",
    "    \"Python is a high-level programming language that emphasizes simplicity and readability.\",\n",
    "    \"Java is a versatile, object-oriented programming language designed for portability across platforms.\",\n",
    "    \"Python is dynamically typed and concise, while Java is statically typed and verbose.\",\n",
    "    \"RAG combines retrieval from a knowledge base with a generator model to produce grounded answers.\"\n",
    "]\n",
    "\n",
    "chunk_embeddings = embedder.encode(knowledge_chunks, convert_to_numpy=True)\n",
    "d = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "\n",
    "\n",
    "def rag_tool(query, k=top_k):\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    picked = [knowledge_chunks[i] for i in I[0] if 0 <= i < len(knowledge_chunks)]\n",
    "    context = \" \".join(picked)\n",
    "    if DEBUG:\n",
    "        print(\"\\n[DEBUG:RAG] D:\", D, \"I:\", I)\n",
    "        print(\"[DEBUG:RAG] Retrieved:\", picked)\n",
    "    return context if context else \"No relevant context found.\"\n",
    "\n",
    "\n",
    "def calculator_tool(query):\n",
    "    expr = re.findall(r\"[0-9+\\-*/().^]+\", query)\n",
    "    if not expr:\n",
    "        return \"No math expression found.\"\n",
    "    raw = \"\".join(expr).strip()\n",
    "    raw = raw.replace(\"^\", \"**\")\n",
    "    try:\n",
    "        result = eval(raw, {\"__builtins__\": {}}, {})\n",
    "        return f\"The result is {result}.\"\n",
    "    except Exception:\n",
    "        return \"Sorry, I couldn't calculate that.\"\n",
    "\n",
    "\n",
    "def _read_pdf_text(pdf_path: str, max_chars: int = 6000) -> str:\n",
    "    if not HAS_PYPDF2:\n",
    "        return \"PyPDF2 not installed.\"\n",
    "    try:\n",
    "        text_parts = []\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                t = page.extract_text() or \"\"\n",
    "                text_parts.append(t)\n",
    "        text = \"\\n\".join(text_parts)\n",
    "        if not text.strip():\n",
    "            return \"No readable text.\"\n",
    "        return text[:max_chars]\n",
    "    except FileNotFoundError:\n",
    "        return \"Pdf not found at path\"\n",
    "    except Exception as e:\n",
    "        return f\"Something went wrong. {e}\""
   ],
   "id": "c03d3c3a2cb0670b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T17:32:40.701264Z",
     "start_time": "2025-08-23T17:32:40.692009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pdf_tool(query):\n",
    "    m = re.search(r\"(?:pdf:|path=|file=)\\s*([^\\s]+\\.pdf)\", query, re.IGNORECASE)\n",
    "    if not m:\n",
    "        return f\"Pdf not found at path\"\n",
    "    pdf_path = m.group(1)\n",
    "    raw_text = _read_pdf_text(pdf_path)\n",
    "    if DEBUG:\n",
    "        print(\"\\n[DEBUG:RAG] PDF:\", pdf_path)\n",
    "        print(\"[DEBUG:RAG] Text:\", raw_text)\n",
    "    if not isinstance(raw_text, str) or \"error\" in raw_text.lower() or raw_text.startswith(\"not installed\"):\n",
    "        return raw_text\n",
    "    prompt = f\"Summarize the following document in 5-7 bullet points:\\n\\n{raw_text}\\n\\nSummary:\"\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = llm_model.generate(**inputs, max_length=220)\n",
    "    summary = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "TOOLS = {\n",
    "    \"RAG\": rag_tool,\n",
    "    \"CALC\": calculator_tool,\n",
    "    \"PDF\": pdf_tool,\n",
    "}\n",
    "\n",
    "\n",
    "def decide_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask the LLM to choose a tool.\n",
    "    Returns one of: 'RAG', 'CALC', 'PDF'\n",
    "    \"\"\"\n",
    "    instruction = f\"\"\"\n",
    "You are a controller agent. Choose the best tool for the user's question.\n",
    "Available tools:\n",
    "- RAG: knowledge questions about programming, technology, and general facts.\n",
    "- CALC: arithmetic or math expressions.\n",
    "- PDF: when the user asks to read or summarize a PDF. The query will include a path like pdf:./file.pdf or path=./file.pdf\n",
    "\n",
    "Question: {query}\n",
    "Respond with exactly one of: RAG, CALC, PDF\n",
    "\"\"\"\n",
    "    inputs = llm_tokenizer(instruction, return_tensors=\"pt\", truncation=True)\n",
    "    out = llm_model.generate(**inputs, max_length=8)\n",
    "    decision = llm_tokenizer.decode(out[0], skip_special_tokens=True).strip().upper()\n",
    "\n",
    "    # Lightweight guards\n",
    "    if \"PDF\" in decision or re.search(r\"\\.pdf\\b\", query, re.IGNORECASE):\n",
    "        decision = \"PDF\"\n",
    "    elif any(tok in query.lower() for tok in [\"+\", \"-\", \"*\", \"/\", \"^\", \"calculate\", \"eval\"]):\n",
    "        decision = \"CALC\"\n",
    "    elif decision not in TOOLS:\n",
    "        decision = \"RAG\"\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"\\n[DEBUG:AGENT] Decision:\", decision)\n",
    "    return decision\n",
    "\n",
    "\n",
    "def agent(query: str) -> str:\n",
    "    tool_name = decide_tool(query)\n",
    "    tool_fn = TOOLS[tool_name]\n",
    "    tool_result = tool_fn(query)\n",
    "\n",
    "    # For RAG outputs, optionally let LLM compose a concise final answer using the retrieved context.\n",
    "    if tool_name == \"RAG\":\n",
    "        prompt = f\"Use ONLY the context below to answer the question concisely.\\n\\nContext:\\n{tool_result}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "        inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "        out = llm_model.generate(**inputs, max_length=180)\n",
    "        final_answer = llm_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        return final_answer\n",
    "\n",
    "    # For CALC/PDF, the tool_result is already final text.\n",
    "    return tool_result"
   ],
   "id": "4877a7f62e814b02",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T17:32:44.532213Z",
     "start_time": "2025-08-23T17:32:41.537771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"User: What is Python?\")\n",
    "    print(\"Agent:\", agent(\"What is Python?\"))\n",
    "\n",
    "    print(\"\\nUser: Compare Python and Java.\")\n",
    "    print(\"Agent:\", agent(\"Compare Python and Java.\"))\n",
    "\n",
    "    print(\"\\nUser: What is 2 + 5 * 3?\")\n",
    "    print(\"Agent:\", agent(\"What is 2 + 5 * 3?\"))\n",
    "\n",
    "    # Provide a real PDF path you have locally:\n",
    "    # e.g., put 'sample.pdf' in the same folder and run:\n",
    "    print(\"\\nUser: summarize pdf: ./sample.pdf\")\n",
    "    print(\"Agent:\", agent(\"summarize pdf: ./sample.pdf\"))"
   ],
   "id": "54ef0051efc430dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Python?\n",
      "\n",
      "[DEBUG:AGENT] Decision: RAG\n",
      "\n",
      "[DEBUG:RAG] D: [[0.29500717 0.9449347 ]] I: [[0 2]]\n",
      "[DEBUG:RAG] Retrieved: ['Python is a high-level programming language that emphasizes simplicity and readability.', 'Python is dynamically typed and concise, while Java is statically typed and verbose.']\n",
      "Agent: high-level programming language\n",
      "\n",
      "User: Compare Python and Java.\n",
      "\n",
      "[DEBUG:AGENT] Decision: RAG\n",
      "\n",
      "[DEBUG:RAG] D: [[0.43986782 0.7197268 ]] I: [[2 0]]\n",
      "[DEBUG:RAG] Retrieved: ['Python is dynamically typed and concise, while Java is statically typed and verbose.', 'Python is a high-level programming language that emphasizes simplicity and readability.']\n",
      "Agent: Python\n",
      "\n",
      "User: What is 2 + 5 * 3?\n",
      "\n",
      "[DEBUG:AGENT] Decision: CALC\n",
      "Agent: The result is 17.\n",
      "\n",
      "User: summarize pdf: ./sample.pdf\n",
      "\n",
      "[DEBUG:AGENT] Decision: PDF\n",
      "\n",
      "[DEBUG:RAG] PDF: ./sample.pdf\n",
      "[DEBUG:RAG] Text: Quantum mechanics  is the fundamental physical  theory  that describes the behavior of matter \n",
      "and of light; its unusual characteristics typically occur at and below the scale of  atoms .[2]: 1.1  It is \n",
      "the foundation of all  quantum physics , which includes  quantum chemistry , quantum field \n",
      "theory , quantum technology , and  quantum information science . \n",
      "Quantum mechanics can describe many systems that  classical physics  cannot. Classical physics \n",
      "can describe many aspects of nature at an ordinary ( macroscopic  and (optical) microscopic ) \n",
      "scale, but is not sufficient for describing them at very small  submicroscopic  (atomic \n",
      "and subatomic ) scales. Classical mechanics can be derived from quantum mechanics as an \n",
      "approximation that is valid at ordinary scales.[3] \n",
      "Quantum systems have  bound  states that are  quantized  to discrete \n",
      "values  of energy , momentum , angular momentum , and other quantities, in contrast to classical \n",
      "systems where these quantities can be measured continuously. Measurements of quantum \n",
      "systems show characteristics of both  particles  and waves  (wave –particle duality ), and there are \n",
      "limits to how accurately the value of a physical quantity can be predicted prior to its \n",
      "measurement, given a complete set of initial conditions (the  uncertainty principle ). \n",
      "Quantum mechanics arose gradually from theories to explain observations that could not be \n",
      "reconciled with  classical physics , such as  Max Planck 's solution in 1900 to the  black -body \n",
      "radiation  problem, and the correspondence between energy and frequency in  Albert \n",
      "Einstein 's 1905 paper , which explained the  photoelectric effect . These early attempts to \n",
      "understand microscopic phenomena, now known as the \" old quantum theory \", led to the full \n",
      "development of quantum mechanics in the mid -1920s by  Niels Bohr , Erwin Schrödinger , Werner \n",
      "Heisenberg , Max Born , Paul Dirac  and others. The modern theory is formulated in \n",
      "various  specially developed mathematical formalisms . In one of them, a mathematical entity \n",
      "called the  wave function  provides information, in the form of  probability amplitudes , about \n",
      "what measurements of a particle's energy, momentum, and other physical properties may yield.  \n",
      "Overview and fundamental concepts  \n",
      "Quantum mechanics allows the calculation of properties and behaviour of  physical systems . It is \n",
      "typically applied to microscopic systems:  molecules , atoms  and subatomic particles . It has been \n",
      "demonstrated to hold for complex molecules with thousands of atoms,[4] but its application to \n",
      "human beings raises philosophical problems, such as  Wigner's friend , and its application to the \n",
      "universe as a whole remains speculative.[5] Predictions of quantum mechanics have been \n",
      "verified experimentally to an extremely high degree of  accuracy . For example, the refinement of \n",
      "quantum mechanics for the interaction of light and matter, known as  quantum \n",
      "electrodynamics  (QED), has been  shown to agree with experiment  to within 1 part in 1012 when \n",
      "predicting the magnetic properties of an electron.[6] \n",
      "A fundamental feature of the theory is that it usually cannot predict with certainty what will \n",
      "happen, but only gives probabilities. Mathematically, a probability is found by taking the square \n",
      "of the absolute value of a  complex number , known as a probability amplitude. This is known as \n",
      "the Born rule , named after physicist  Max Born . For example, a quantum particle like \n",
      "an electron  can be described by a wave function, which associates to each point in space a \n",
      "probability amplitude. Applying the Born rule to these amplitudes gives a  probability density \n",
      "function  for the position that the electron will be found to have when an experiment is \n",
      "performed to measure it. This is the best the theory can do; it cannot say for certain where the \n",
      "electron will be found. The  Schrödinger equation  relates the collection of probability amplitudes \n",
      "that pertain to one moment of time to the collection of probability amplitudes that pertain to \n",
      "another.[7]: 67–87  \n",
      "One consequence of the mathematical rules of quantum mechanics is a tradeoff in \n",
      "predictability between measurable quantities. The most famous form of this  uncertainty \n",
      "principle  says that no matter how a quantum particle is prepared or how carefully experiments \n",
      "upon it are arranged, it is impossible to have a precise prediction for a measurement of its \n",
      "position and also at the same time for a measurement of its  momentum .[7]: 427 –435  \n",
      " \n",
      "Agent: Quantum mechanics is the fundamental physical theory that describes the behavior of matter and of light.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "6b7abb7fd0c67cad",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "963b4b1c4debea8e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e1b1e671204fe777",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "314c7406bee89559",
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T19:40:25.760799Z",
     "start_time": "2025-08-23T19:40:25.750147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "- Advanced chunking (sliding window + simple merging)\n",
    "- Dense retrieval with FAISS (sentence-transformers)\n",
    "- Sparse retrieval with BM25 (rank_bm25)\n",
    "- Hybrid retrieval combining both\n",
    "- Cross-encoder reranking (sentence-transformers CrossEncoder)\n",
    "- Simple evaluation helpers & debug logging\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "# models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# LLM part for generation (optional; you can plug your generator)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG (tweak these)\n",
    "# ---------------------------\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"  # embeddings (dense)\n",
    "RERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # cross-encoder\n",
    "QA_LLM = \"google/flan-t5-base\"  # used only for final answer (optional)\n",
    "CHUNK_TOKENS = 50  # approx tokens per chunk\n",
    "CHUNK_OVERLAP = 30  # sliding-window overlap\n",
    "TOP_K_DENSE = 10\n",
    "TOP_K_SPARSE = 10\n",
    "HYBRID_K = 10  # union size before rerank\n",
    "FINAL_TOPK = 3  # top-k after rerank -> feed LLM\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# UTIL: simple tokenizer (whitespace)\n",
    "# ---------------------------\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return [t for t in text.split() if t.strip()]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Advanced chunking\n",
    "#    - sliding window on tokens (approx)\n",
    "#    - merge very short lines with next line (basic heading handling)\n",
    "# ---------------------------\n",
    "def chunk_text_advanced(text: str,\n",
    "                        chunk_tokens: int = CHUNK_TOKENS,\n",
    "                        overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "\n",
    "    # Preprocess lines: merge short heading-like lines with next line\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    merged_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        # heuristic: short line (<=6 tokens) likely heading -> merge with next\n",
    "        if len(simple_tokenize(line)) <= 6 and (i + 1) < len(lines):\n",
    "            merged = line + \" \" + lines[i + 1]\n",
    "            merged_lines.append(merged)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_lines.append(line)\n",
    "            i += 1\n",
    "\n",
    "    # join into one token list, but keep mapping to text spans\n",
    "    tokens = []\n",
    "    token_to_text = []\n",
    "    for chunk in merged_lines:\n",
    "        tks = simple_tokenize(chunk)\n",
    "        for tk in tks:\n",
    "            tokens.append(tk)\n",
    "            token_to_text.append(chunk)  # reference original chunk for context reconstruction\n",
    "\n",
    "    # sliding window\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(tokens)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_tokens, n)\n",
    "        # reconstruct approximate text span from token_to_text\n",
    "        slice_texts = token_to_text[start:end]\n",
    "        # to avoid too repetitive repeats, take unique contiguous\n",
    "        out_lines = []\n",
    "        prev = None\n",
    "        for s in slice_texts:\n",
    "            if s != prev:\n",
    "                out_lines.append(s)\n",
    "            prev = s\n",
    "        chunk_text = \" \".join(out_lines)\n",
    "        chunks.append(chunk_text)\n",
    "        if end == n:\n",
    "            break\n",
    "        start += (chunk_tokens - overlap)\n",
    "    # deduplicate near-identical chunks while preserving order\n",
    "    seen = set()\n",
    "    final_chunks = []\n",
    "    for c in chunks:\n",
    "        key = c[:200]  # rough key\n",
    "        if key not in seen:\n",
    "            final_chunks.append(c)\n",
    "            seen.add(key)\n",
    "    return final_chunks"
   ],
   "id": "5d1e91a1acea7429",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T19:40:26.410758Z",
     "start_time": "2025-08-23T19:40:26.400637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 2) Build dense embeddings + FAISS index\n",
    "# ---------------------------\n",
    "def build_faiss_index(chunks: List[str], embedder: SentenceTransformer):\n",
    "    embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "    # ensure dtype float32\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)  # simple exact index; replace with HNSW/IVF for big corpora\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Build sparse index (BM25)\n",
    "# ---------------------------\n",
    "def build_bm25_index(chunks: List[str]):\n",
    "    tokenized = [simple_tokenize(c.lower()) for c in chunks]\n",
    "    bm25 = BM25Okapi(tokenized)\n",
    "    return bm25, tokenized\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Hybrid retrieval: dense top-k + sparse top-k union\n",
    "# ---------------------------\n",
    "def hybrid_retrieve(query: str,\n",
    "                    chunks: List[str],\n",
    "                    embedder: SentenceTransformer,\n",
    "                    faiss_index,\n",
    "                    bm25_index,\n",
    "                    tokenized_chunks,\n",
    "                    top_k_dense: int = TOP_K_DENSE,\n",
    "                    top_k_sparse: int = TOP_K_SPARSE,\n",
    "                    hybrid_k: int = HYBRID_K\n",
    "                    ) -> Tuple[List[int], Dict[int, Dict]]:\n",
    "    # dense search\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True).astype('float32')\n",
    "    D, I = faiss_index.search(q_emb, top_k_dense)\n",
    "    dense_indices = [int(i) for i in I[0] if i >= 0]\n",
    "\n",
    "    # sparse\n",
    "    sparse_scores = bm25_index.get_scores(simple_tokenize(query.lower()))\n",
    "    sparse_ranked = np.argsort(-np.array(sparse_scores))[:top_k_sparse].tolist()\n",
    "\n",
    "    # union preserving order of closeness (we'll weight later)\n",
    "    candidate_set = []\n",
    "    for idx in dense_indices + sparse_ranked:\n",
    "        if idx not in candidate_set and 0 <= idx < len(chunks):\n",
    "            candidate_set.append(idx)\n",
    "\n",
    "    # if not enough candidates, pad from full list\n",
    "    if len(candidate_set) < hybrid_k:\n",
    "        for i in range(len(chunks)):\n",
    "            if i not in candidate_set:\n",
    "                candidate_set.append(i)\n",
    "            if len(candidate_set) >= hybrid_k:\n",
    "                break\n",
    "\n",
    "    # prepare candidate metadata\n",
    "    metadata = {}\n",
    "    for idx in candidate_set:\n",
    "        metadata[idx] = {\n",
    "            \"chunk\": chunks[idx],\n",
    "            \"dense_rank\": dense_indices.index(idx) if idx in dense_indices else None,\n",
    "            \"sparse_rank\": sparse_ranked.index(idx) if idx in sparse_ranked else None\n",
    "        }\n",
    "    return candidate_set[:hybrid_k], metadata"
   ],
   "id": "a12e6e78f8dccdfa",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T19:40:27.095255Z",
     "start_time": "2025-08-23T19:40:27.083658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 5) Cross-encoder reranker\n",
    "# ---------------------------\n",
    "def rerank_with_cross_encoder(query: str, candidate_indices: List[int], chunks: List[str], reranker: CrossEncoder,\n",
    "                              top_k: int = FINAL_TOPK):\n",
    "    pairs = [(query, chunks[i]) for i in candidate_indices]\n",
    "    scores = reranker.predict(pairs)  # returns list of relevance scores\n",
    "    # pair indices and scores\n",
    "    scored = list(zip(candidate_indices, scores))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    top = scored[:top_k]\n",
    "    top_indices = [idx for idx, sc in top]\n",
    "    top_scores = [sc for idx, sc in top]\n",
    "    return top_indices, top_scores\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Prompt builder & generator (LLM)\n",
    "# ---------------------------\n",
    "def build_prompt_with_context(query: str, contexts: List[str], history: List[Tuple[str, str]] = None):\n",
    "    history_text = \"\"\n",
    "    if history:\n",
    "        for u, b in history[-3:]:\n",
    "            history_text += f\"User: {u}\\nBot: {b}\\n\"\n",
    "    context_block = \"\\n\\n\".join(contexts)\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use ONLY the context below to answer the question concisely.\n",
    "Conversation so far:\n",
    "{history_text}\n",
    "\n",
    "Context:\n",
    "{context_block}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer_via_llm(prompt: str, qa_tokenizer, qa_model, max_new_tokens: int = 200):\n",
    "    inputs = qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = qa_model.generate(**inputs, max_length=max_new_tokens)\n",
    "    ans = qa_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return ans\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Simple evaluation helper (precision@k) - requires gold_indices per query\n",
    "# ---------------------------\n",
    "def precision_at_k(retrieved_indices: List[int], gold_indices: List[int], k: int):\n",
    "    retrieved_topk = retrieved_indices[:k]\n",
    "    if not gold_indices:\n",
    "        return 0.0\n",
    "    hits = sum(1 for r in retrieved_topk if r in gold_indices)\n",
    "    return hits / min(k, len(retrieved_topk))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage (run pipeline)\n",
    "# ---------------------------\n",
    "def demo_pipeline(corpus_text: str, queries: List[Dict], debug: bool = True):\n",
    "    \"\"\"\n",
    "    corpus_text: big text to chunk\n",
    "    queries: list of dict { \"q\": \"query text\", \"gold\": [list of gold chunk indices] (optional) }\n",
    "    \"\"\"\n",
    "    # 1 chunking\n",
    "    chunks = chunk_text_advanced(corpus_text)\n",
    "    if debug:\n",
    "        print(f\"[INFO] Created {len(chunks)} chunks from corpus.\")\n",
    "\n",
    "    # 2 build dense index\n",
    "    embedder = SentenceTransformer(EMBED_MODEL)\n",
    "    faiss_index, dense_embeddings = None, None\n",
    "    faiss_index, dense_embeddings = build_faiss_index(chunks, embedder)\n",
    "\n",
    "    # 3 bm25\n",
    "    bm25, tokenized_chunks = build_bm25_index(chunks)\n",
    "\n",
    "    # 4 reranker model\n",
    "    reranker = CrossEncoder(RERANKER_MODEL)\n",
    "\n",
    "    # 5 QA LLM (optional)\n",
    "    qa_tokenizer = AutoTokenizer.from_pretrained(QA_LLM)\n",
    "    qa_model = AutoModelForSeq2SeqLM.from_pretrained(QA_LLM)\n",
    "\n",
    "    results = []\n",
    "    for qq in queries:\n",
    "        qtext = qq[\"q\"]\n",
    "        gold = qq.get(\"gold\", None)\n",
    "\n",
    "        # hybrid retrieve\n",
    "        cand_idxs, meta = hybrid_retrieve(qtext, chunks, embedder, faiss_index, bm25, tokenized_chunks)\n",
    "        if debug:\n",
    "            print(\"\\n[DEBUG] Hybrid candidates:\", cand_idxs)\n",
    "            for ci in cand_idxs:\n",
    "                print(f\" - idx {ci}: {chunks[ci][:140]}...\")\n",
    "\n",
    "        # rerank\n",
    "        top_idxs, top_scores = rerank_with_cross_encoder(qtext, cand_idxs, chunks, reranker)\n",
    "        if debug:\n",
    "            print(\"[DEBUG] Reranked top:\", list(zip(top_idxs, top_scores)))\n",
    "\n",
    "        # build prompt with top contexts\n",
    "        top_contexts = [chunks[i] for i in top_idxs]\n",
    "        prompt = build_prompt_with_context(qtext, top_contexts)\n",
    "        answer = generate_answer_via_llm(prompt, qa_tokenizer, qa_model)\n",
    "\n",
    "        # evaluation if gold provided\n",
    "        p_at_3 = None\n",
    "        if gold is not None:\n",
    "            p_at_3 = precision_at_k(top_idxs, gold, 3)\n",
    "\n",
    "        results.append({\n",
    "            \"query\": qtext,\n",
    "            \"top_indices\": top_idxs,\n",
    "            \"top_scores\": top_scores,\n",
    "            \"answer\": answer,\n",
    "            \"precision@3\": p_at_3\n",
    "        })\n",
    "        if debug:\n",
    "            print(\"[RESULT ANSWER]\", answer)\n",
    "            if p_at_3 is not None:\n",
    "                print(\"[EVAL] precision@3:\", p_at_3)\n",
    "\n",
    "    return results"
   ],
   "id": "1b094679edb86f8a",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T19:40:42.308017Z",
     "start_time": "2025-08-23T19:40:27.864024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ---------------------------\n",
    "# If run as script: demo with sample text\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    sample_corpus = \"\"\"\n",
    "Python is a high-level programming language that emphasizes simplicity and readability.\n",
    "It is dynamically typed and widely used for scripting, web development, and data science.\n",
    "\n",
    "Java is a versatile, object-oriented programming language designed for portability across platforms.\n",
    "It is statically typed and commonly used in enterprise applications and Android development.\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) combines retrieval from a knowledge base with a generator model to produce grounded answers.\n",
    "\"\"\"\n",
    "    qs = [\n",
    "        {\"q\": \"What is Python?\"},\n",
    "        {\"q\": \"Describe Java and where it is used.\"},\n",
    "        {\"q\": \"What is RAG?\"}\n",
    "    ]\n",
    "    demo_pipeline(sample_corpus, qs, debug=True)\n"
   ],
   "id": "d9833c20ad8a092e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created 2 chunks from corpus.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "589ac8a2d3974bc5a5453b2e9c19a5ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Hybrid candidates: [0, 1]\n",
      " - idx 0: Python is a high-level programming language that emphasizes simplicity and readability. It is dynamically typed and widely used for scriptin...\n",
      " - idx 1: It is dynamically typed and widely used for scripting, web development, and data science. Java is a versatile, object-oriented programming l...\n",
      "[DEBUG] Reranked top: [(0, 9.42434), (1, -4.8227196)]\n",
      "[RESULT ANSWER] a high-level programming language\n",
      "\n",
      "[DEBUG] Hybrid candidates: [1, 0]\n",
      " - idx 1: It is dynamically typed and widely used for scripting, web development, and data science. Java is a versatile, object-oriented programming l...\n",
      " - idx 0: Python is a high-level programming language that emphasizes simplicity and readability. It is dynamically typed and widely used for scriptin...\n",
      "[DEBUG] Reranked top: [(1, 5.878658), (0, 4.2013035)]\n",
      "[RESULT ANSWER] Java is a versatile, object-oriented programming language designed for portability across platforms. It is statically typed and commonly used in enterprise applications and Android development.\n",
      "\n",
      "[DEBUG] Hybrid candidates: [1, 0]\n",
      " - idx 1: It is dynamically typed and widely used for scripting, web development, and data science. Java is a versatile, object-oriented programming l...\n",
      " - idx 0: Python is a high-level programming language that emphasizes simplicity and readability. It is dynamically typed and widely used for scriptin...\n",
      "[DEBUG] Reranked top: [(1, 4.206398), (0, -9.560638)]\n",
      "[RESULT ANSWER] Retrieval-Augmented Generation\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3a20cd1840b4a7c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
