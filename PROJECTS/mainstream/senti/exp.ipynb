{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================\n",
    "# Toxic Comment Classifier (From Scratch)\n",
    "#  - GRU + Attention\n",
    "#  - BiLSTM + Attention\n",
    "#  - Mini-Transformer (no pretrain)\n",
    "# Colab-ready, no external deps beyond sklearn\n",
    "# ============================\n",
    "\n",
    "# ===== 0) Imports & Setup =====\n",
    "import os, re, html, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ===== 1) Config =====\n",
    "CSV_PATH = \"/content/train.csv\"   # <-- change to your file path in Colab/Drive\n",
    "TEXT_COL = \"comment_text\"\n",
    "LABEL_COLS = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "# Choose one: \"gru\", \"lstm\", or \"transformer\"\n",
    "MODEL_TYPE = \"gru\"                 # <-- set here\n",
    "MAX_LEN = 128\n",
    "MIN_FREQ = 2                       # min token frequency for vocab\n",
    "BATCH_SIZE = 64                    # bump on Colab; reduce on small GPU\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "D_MODEL = 256                      # for transformer\n",
    "EPOCHS = 8\n",
    "LR = 2e-3 if MODEL_TYPE != \"transformer\" else 1e-3\n",
    "PATIENCE = 2\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# ===== 2) Load Data =====\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert TEXT_COL in df.columns, f\"'{TEXT_COL}' not in CSV columns\"\n",
    "for c in LABEL_COLS:\n",
    "    assert c in df.columns, f\"'{c}' not in CSV\"\n",
    "\n",
    "print(df[TEXT_COL].head())\n",
    "\n",
    "# ===== 3) Basic Cleaning & Tokenization =====\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text)\n",
    "    text = html.unescape(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s']\", \" \", text)  # keep simple\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text: str):\n",
    "    # whitespace + keep apostrophes for contractions\n",
    "    return clean_text(text).split()\n",
    "\n",
    "# Build vocab\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\"]\n",
    "counter = Counter()\n",
    "for t in tqdm(df[TEXT_COL].astype(str), desc=\"Building vocab\"):\n",
    "    counter.update(simple_tokenize(t))\n",
    "\n",
    "itos = special_tokens + [tok for tok, freq in counter.items() if freq >= MIN_FREQ and tok not in special_tokens]\n",
    "stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "PAD_IDX = stoi[\"<PAD>\"]; UNK_IDX = stoi[\"<UNK>\"]\n",
    "vocab_size = len(itos)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "def encode(text, max_len=MAX_LEN):\n",
    "    toks = simple_tokenize(text)\n",
    "    ids = [stoi.get(tok, UNK_IDX) for tok in toks[:max_len]]\n",
    "    if len(ids) < max_len:\n",
    "        ids = ids + [PAD_IDX] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "# Vectorize the whole dataset\n",
    "all_ids = np.vstack([encode(t) for t in tqdm(df[TEXT_COL].astype(str), desc=\"Encoding texts\")])\n",
    "all_labels = df[LABEL_COLS].astype('float32').values\n",
    "\n",
    "# ===== 4) Train/Val/Test Split =====\n",
    "N = len(df)\n",
    "idxs = np.arange(N)\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "train_ratio, val_ratio = 0.8, 0.1\n",
    "n_train = int(train_ratio * N)\n",
    "n_val = int(val_ratio * N)\n",
    "train_idx = idxs[:n_train]\n",
    "val_idx = idxs[n_train:n_train+n_val]\n",
    "test_idx = idxs[n_train+n_val:]\n",
    "\n",
    "X_train = torch.tensor(all_ids[train_idx], dtype=torch.long)\n",
    "y_train = torch.tensor(all_labels[train_idx], dtype=torch.float32)\n",
    "X_val   = torch.tensor(all_ids[val_idx], dtype=torch.long)\n",
    "y_val   = torch.tensor(all_labels[val_idx], dtype=torch.float32)\n",
    "X_test  = torch.tensor(all_ids[test_idx], dtype=torch.long)\n",
    "y_test  = torch.tensor(all_labels[test_idx], dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"Splits -> train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "\n",
    "# ===== 5) Models =====\n",
    "class AttnPool(nn.Module):\n",
    "    # simple additive attention over sequence outputs\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, seq_out, mask=None):\n",
    "        # seq_out: (B, T, C)\n",
    "        scores = self.attn(seq_out).squeeze(-1)  # (B, T)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "        weights = torch.softmax(scores, dim=1)    # (B, T)\n",
    "        pooled = torch.bmm(weights.unsqueeze(1), seq_out).squeeze(1)  # (B, C)\n",
    "        return pooled, weights\n",
    "\n",
    "class GRUAttnClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn = AttnPool(hidden_dim*2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T)\n",
    "        mask = (x != PAD_IDX).long()\n",
    "        e = self.emb(x)                            # (B,T,E)\n",
    "        y, _ = self.gru(e)                         # (B,T,2H)\n",
    "        pooled, _ = self.attn(y, mask=mask)        # (B,2H)\n",
    "        out = self.dropout(pooled)\n",
    "        return self.fc(out)                        # logits (B, L)\n",
    "\n",
    "class LSTMAttnClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn = AttnPool(hidden_dim*2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != PAD_IDX).long()\n",
    "        e = self.emb(x)\n",
    "        y, _ = self.lstm(e)\n",
    "        pooled, _ = self.attn(y, mask=mask)\n",
    "        out = self.dropout(pooled)\n",
    "        return self.fc(out)\n",
    "\n",
    "# Positional encoding for Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, T, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class MiniTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, num_labels, pad_idx=0, dim_ff=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_ff, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T)\n",
    "        key_padding_mask = (x == PAD_IDX)  # True to mask\n",
    "        e = self.emb(x)                    # (B,T,C)\n",
    "        e = self.pos(e)\n",
    "        h = self.encoder(e, src_key_padding_mask=key_padding_mask)\n",
    "        # Pooling: mean over non-pad tokens\n",
    "        mask = (~key_padding_mask).unsqueeze(-1)   # (B,T,1)\n",
    "        h_masked = h * mask\n",
    "        summed = h_masked.sum(dim=1)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)\n",
    "        pooled = summed / lengths\n",
    "        out = self.dropout(pooled)\n",
    "        return self.fc(out)                # logits (B, L)\n",
    "\n",
    "# Build the chosen model\n",
    "num_labels = len(LABEL_COLS)\n",
    "if MODEL_TYPE == \"gru\":\n",
    "    model = GRUAttnClassifier(vocab_size, EMBED_DIM, HIDDEN_DIM, num_labels, pad_idx=PAD_IDX)\n",
    "elif MODEL_TYPE == \"lstm\":\n",
    "    model = LSTMAttnClassifier(vocab_size, EMBED_DIM, HIDDEN_DIM, num_labels, pad_idx=PAD_IDX)\n",
    "elif MODEL_TYPE == \"transformer\":\n",
    "    model = MiniTransformerClassifier(vocab_size, D_MODEL, num_heads=4, num_layers=2, num_labels=num_labels, pad_idx=PAD_IDX, dim_ff=512)\n",
    "else:\n",
    "    raise ValueError(\"MODEL_TYPE must be 'gru', 'lstm', or 'transformer'.\")\n",
    "\n",
    "model = model.to(device)\n",
    "print(model.__class__.__name__, \"params:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n",
    "\n",
    "# ===== 6) Train Utilities =====\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
    "\n",
    "# Reduce LR on plateau (simple & robust)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "def run_epoch(dataloader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    for X, y in tqdm(dataloader, leave=False):\n",
    "        X = X.to(device); y = y.to(device)\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate_thresholded(dataloader, threshold=THRESHOLD):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "            all_labels.append(y.numpy())\n",
    "    probs = np.vstack(all_probs)\n",
    "    y_true = np.vstack(all_labels)\n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "\n",
    "    # Per-label metrics\n",
    "    per_label = {}\n",
    "    for i, label in enumerate(LABEL_COLS):\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true[:, i], probs[:, i])\n",
    "        except ValueError:\n",
    "            auc = float('nan')\n",
    "        per_label[label] = {\n",
    "            \"accuracy\": float(accuracy_score(y_true[:, i], y_pred[:, i])),\n",
    "            \"precision\": float(precision_score(y_true[:, i], y_pred[:, i], zero_division=0)),\n",
    "            \"recall\": float(recall_score(y_true[:, i], y_pred[:, i], zero_division=0)),\n",
    "            \"f1\": float(f1_score(y_true[:, i], y_pred[:, i], zero_division=0)),\n",
    "            \"roc_auc\": float(auc),\n",
    "        }\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    return per_label, macro_f1, micro_f1\n",
    "\n",
    "# ===== 7) Train Loop (with Early Stopping) =====\n",
    "best_val = float('inf')\n",
    "bad_epochs = 0\n",
    "BEST_PATH = f\"best_{MODEL_TYPE}_toxicity.pt\"\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS} (lr={optimizer.param_groups[0]['lr']:.2e})\")\n",
    "    train_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss   = run_epoch(val_loader, train=False)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\" Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val - 1e-4:\n",
    "        best_val = val_loss\n",
    "        bad_epochs = 0\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print(\"  ✅ Improved, model saved.\")\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        print(f\"  ⚠ No improvement ({bad_epochs}/{PATIENCE})\")\n",
    "        if bad_epochs >= PATIENCE:\n",
    "            print(\"  ⛔ Early stopping.\")\n",
    "            break\n",
    "\n",
    "# Load best\n",
    "model.load_state_dict(torch.load(BEST_PATH, weights_only=False))\n",
    "model.eval();\n",
    "\n",
    "# ===== 8) Final Evaluation =====\n",
    "per_label, macro_f1, micro_f1 = evaluate_thresholded(test_loader, threshold=THRESHOLD)\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "for k, v in per_label.items():\n",
    "    print(f\"{k:14s}  acc={v['accuracy']:.3f}  prec={v['precision']:.3f}  rec={v['recall']:.3f}  f1={v['f1']:.3f}  auc={v['roc_auc']:.3f}\")\n",
    "print(f\"\\nMacro F1: {macro_f1:.3f} | Micro F1: {micro_f1:.3f}\")\n",
    "\n",
    "# ===== 9) Inference on your own text =====\n",
    "def predict_comment(text, threshold=THRESHOLD):\n",
    "    ids = torch.tensor([encode(text)], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(ids)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    print(\"\\nComment:\", text)\n",
    "    for label, p, z in zip(LABEL_COLS, probs, preds):\n",
    "        print(f\"{label:14s}: {'Yes' if z==1 else 'No '} (prob: {p:.2f})\")\n",
    "    return {label: float(p) for label, p in zip(LABEL_COLS, probs)}\n",
    "\n",
    "# Example:\n",
    "# predict_comment(\"I hate you and you are awful!\")\n"
   ],
   "id": "e259618a9c5987ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
