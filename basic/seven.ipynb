{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# rag_ollama.py\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ---------- Config ----------\n",
    "DOCS_DIR = \"docs/\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "INDEX_PATH = \"faiss_index.bin\"\n",
    "META_PATH = \"faiss_meta.json\"\n",
    "TOP_K = 4\n",
    "\n",
    "\n",
    "# ---------- Helpers: load & chunk documents ----------\n",
    "def load_text_files(folder: str) -> List[Tuple[str, str]]:\n",
    "    files = glob.glob(os.path.join(folder, \"*.txt\"))\n",
    "    docs = []\n",
    "    for p in files:\n",
    "        name = os.path.basename(p)\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        docs.append((name, text))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ---------- Build embeddings + FAISS index ----------\n",
    "def build_faiss_index(docs_dir: str):\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    docs = load_text_files(docs_dir)\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "    for doc_id, text in docs:\n",
    "        chunks = chunk_text(text, chunk_size=200, overlap=40)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            emb = model.encode(chunk, convert_to_numpy=True)\n",
    "            embeddings.append(emb)\n",
    "            metadata.append({\"doc_id\": doc_id, \"chunk_id\": i, \"text\": chunk})\n",
    "    if not embeddings:\n",
    "        raise ValueError(\"No document chunks found. Put .txt files in docs/\")\n",
    "\n",
    "    embeddings = np.vstack(embeddings).astype(\"float32\")\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    faiss.write_index(index, INDEX_PATH)\n",
    "    with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Built index with {index.ntotal} vectors, saved to {INDEX_PATH}\")\n",
    "\n",
    "\n",
    "# ---------- Load index + retrieve ----------\n",
    "def load_index():\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "    return index, metadata\n",
    "\n",
    "\n",
    "def retrieve(query: str, index, metadata, model: SentenceTransformer, k: int = TOP_K):\n",
    "    q_emb = model.encode(query, convert_to_numpy=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb.reshape(1, -1))\n",
    "    scores, ids = index.search(q_emb.reshape(1, -1), k)\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], ids[0]):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        meta = metadata[idx]\n",
    "        results.append({\"score\": float(score), \"meta\": meta})\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------- Ollama generation ----------\n",
    "\n",
    "def generate_with_ollama(query: str, retrieved_chunks: list):\n",
    "    context = \"\\n\\n---\\n\\n\".join([c[\"meta\"][\"text\"] for c in retrieved_chunks])\n",
    "    prompt = (\n",
    "        f\"You are a helpful assistant. Use the following context to answer the question. \"\n",
    "        f\"If the answer is not in the context, say 'I don't know'.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    response = ollama.generate(\n",
    "        model=\"llama3.1:8b\",  # your local model\n",
    "        prompt=prompt\n",
    "    )\n",
    "    return response[\"response\"].strip()\n",
    "\n",
    "\n",
    "# ---------- Simple interactive function ----------\n",
    "def answer_query(query: str):\n",
    "    index, metadata = load_index()\n",
    "    embed_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    retrieved = retrieve(query, index, metadata, embed_model, k=TOP_K)\n",
    "    print(\"Retrieved chunks (score, doc_id, chunk_id):\")\n",
    "    for r in retrieved:\n",
    "        print(r[\"score\"], r[\"meta\"][\"doc_id\"], r[\"meta\"][\"chunk_id\"])\n",
    "    return generate_with_ollama(query, retrieved)\n",
    "\n",
    "\n",
    "# ---------- CLI ----------\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--build\", action=\"store_true\", help=\"Build index from docs/\")\n",
    "    parser.add_argument(\"--query\", type=str, help=\"Query text\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.build:\n",
    "        build_faiss_index(DOCS_DIR)\n",
    "    elif args.query:\n",
    "        resp = answer_query(args.query)\n",
    "        print(\"\\n=== Answer ===\\n\")\n",
    "        print(resp)\n",
    "    else:\n",
    "        print(\"Run with --build to build index, then --query 'your question' to ask.\")"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "ac34725dccff435f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "a1a677edb32cff2c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
